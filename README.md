# ğŸš€ SEAL-Lite: Lightweight Continual Learning Framework

A Final-Year Project on Overcoming Catastrophic Forgetting Using Adaptive Editing, Replay Memory, and Utility-Guided Learning

## ğŸ“˜ Overview
SEAL-Lite is a lightweight continual learning system inspired by the research paper: Self-Edit Accumulation Learning (SEAL) (2025).

Traditional neural networks forget previously learned tasks when trained sequentially â€” a phenomenon called catastrophic forgetting. SEAL-Lite aims to solve this problem using:
- Adaptive edits (local or LLM-generated)
- Utility-based scoring
- Priority replay memory
- Self-supervised continual fine-tuning

The framework is fully CPU-compatible, uses DistilBERT, and supports local LLMs via Ollama.

## ğŸ“‹ Table of Contents
- [Features](#-features)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [How It Works](#-how-it-works)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸ§  Key Features

### âœ” Continual Learning Engine
- Based on SEAL's edit â†’ score â†’ store â†’ replay cycle
- Prevents catastrophic forgetting with replay + utility signals

### âœ” Dual Edit Modes
- Local rule-based editor (fast, offline)
- LLM-powered editor via Ollama (optional)

### âœ” Replay Memory
- JSONL-backed memory
- Priority sampling
- Stores high-utility edits

### âœ” Utility Scoring
- Measures how much each edit improves model confidence
- Ensures valuable edits stay in memory

### âœ” Trainer
- Batch training
- Confidence-based predictions
- CPU-only support

### âœ” Modular Architecture
Clear separation of:
- memory
- utility
- replay
- trainer
- prompt adapter
- LLM adapter
- runner

## ğŸ›  Installation

1. **Create environment**
   ```bash
   conda create -n seal_env python=3.10
   conda activate seal_env
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **(Optional) Run Ollama**
   ```bash
   ollama serve
   ```
   Pull a model:
   ```bash
   ollama pull llama2
   ```

## ğŸ Running the Project

### SEAL Continual Learning Loop
```bash
python main.py --mode seal
```
Runs 1000+ continual updates with memory + replay.

### IMDB Local vs LLM Comparison
```bash
python main.py --mode imdb
```
Shows performance difference between local edit mode and LLM edit mode.

### Lightweight Demonstration
```bash
python main.py --mode demo
```
Runs SEAL on 3 IMDB samples.

### Future: Multi-Task Sequential Training
```bash
python main.py --mode tasks
```
(Placeholder for IMDB â†’ SQuAD â†’ ARC sequence)

## ğŸ“‚ Project Structure

```
SEAL/
â”‚â”€â”€ main.py
â”‚â”€â”€ configs/
â”‚     â””â”€â”€ default.yaml
â”‚â”€â”€ seal/
â”‚     â”œâ”€â”€ runner.py
â”‚     â”œâ”€â”€ trainer.py
â”‚     â”œâ”€â”€ adapter.py
â”‚     â”œâ”€â”€ llm_adapter.py
â”‚     â”œâ”€â”€ prompt_adapter.py
â”‚     â”œâ”€â”€ memory.py
â”‚     â”œâ”€â”€ replay.py
â”‚     â”œâ”€â”€ utility.py
â”‚     â””â”€â”€ demo_seal.py
â”‚â”€â”€ memory/
â”‚     â””â”€â”€ edits.jsonl
â”‚â”€â”€ outputs/
â”‚     â”œâ”€â”€ plots
â”‚     â””â”€â”€ results
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Edit `configs/default.yaml` to customize:

```yaml
# Model configuration
model:
  name: distilbert-base-uncased
  num_labels: 2

# Training parameters
trainer:
  learning_rate: 1e-4
  max_steps: 1000
  eval_interval: 10
  device: cpu  # or cuda

# Memory and replay
memory:
  path: "memory/edits.jsonl"
  max_size: 20000

replay:
  batch_size: 8
  replay_fraction: 0.5
  policy: "priority"  # or "uniform"

# LLM configuration (for LLM editing mode)
llm:
  backend: "ollama"  # or "openai"
  model: "llama2"
  timeout: 120
```

## âš¡ How SEAL-Lite Works

1. **Take a sample**
   From IMDB, SQuAD (future), or ARC (future).

2. **Generate an edit**
   - Local rule-based sentiment edit
   - OR LLM edit (via Ollama)

3. **Score utility**
   Measure improvement in model confidence.

4. **Store edit in memory**
   Only high-utility edits remain.

5. **Replay + new data â†’ batch**
   Balance between stability and plasticity.

6. **Train**
   DistilBERT is updated gradually.

## ğŸ› Troubleshooting

### Common Issues

1. **LLM Connection Errors**
   - Ensure Ollama server is running for LLM mode
   - Check API keys for OpenAI backend

2. **CUDA Out of Memory**
   - Reduce batch size in config
   - Set `device: cpu` if GPU memory is limited

3. **Installation Issues**
   - Use Python 3.9 or higher
   - Create a fresh virtual environment

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a pull request

## ğŸ“Š Outputs
Generated under `/outputs/`:
- Training loss curve
- Accuracy curve
- JSON results
- System resource logs
- Comparison summaries (local vs llm)

## ğŸ¯ Project Goals
- Build a resource-friendly continual learning system
- Demonstrate SEAL-style self-edit adaptive learning
- Overcome catastrophic forgetting
- Integrate local & LLM-based edits
- Achieve high retention across tasks

## ğŸ“š References
- SEAL: Self-Edit Accumulation Learning (2025)
- Continual Learning literature
- HuggingFace Transformers
- Ollama Local LLM runtime

## ğŸ“ Future Enhancements
- Multi-task continual evaluation (IMDB â†’ SQuAD â†’ ARC)
- Visualization dashboard (Streamlit)
- GPU training support
- Edit-quality analysis
- Memory pruning strategies

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
