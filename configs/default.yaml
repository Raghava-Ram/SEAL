# SEAL Configuration
# This is the default configuration file for SEAL (Self-Edit Adaptive Learning)

# LLM Configuration
llm:
  # Backend to use for LLM (ollama or openai)
  backend: "ollama"

# Ollama Configuration
ollama:
  # Base URL of the Ollama server
  host: "http://localhost:11434"
  # Model to use (e.g., "qwen3:4b")
  model: "llama2"
  # Enable streaming responses
  stream: true
  # Request timeout in seconds
  timeout: 30
  # Maximum number of retries for failed requests
  max_retries: 3

# OpenAI Configuration (for backward compatibility)
openai:
  # Your OpenAI API key
  api_key: ""
  # Model to use (e.g., "gpt-3.5-turbo")
  model: "gpt-3.5-turbo"

# Model configuration
model:
  # Model name or path
  name_or_path: "distilbert-base-uncased"
  # Number of labels for classification
  num_labels: 2
  # Dropout probability
  dropout: 0.1

# Training configuration
training:
  # Batch size for training
  per_device_train_batch_size: 16
  # Batch size for evaluation
  per_device_eval_batch_size: 16
  # Number of training epochs
  num_train_epochs: 3
  # Learning rate
  learning_rate: 2e-5
  # Weight decay
  weight_decay: 0.01
  # Warmup steps
  warmup_steps: 500
  # Maximum gradient norm (for gradient clipping)
  max_grad_norm: 1.0
  # Logging steps
  logging_steps: 100
  # Evaluation strategy (steps, epoch)
  evaluation_strategy: "epoch"
  # Save strategy (steps, epoch)
  save_strategy: "epoch"
  # Save total limit
  save_total_limit: 1
  # Load best model at end
  load_best_model_at_end: true
  # Metric for best model
  metric_for_best_model: "accuracy"
  # Greater is better
  greater_is_better: true

# Data configuration
data:
  # Maximum sequence length
  max_length: 128
  # Text column name
  text_column: "text"
  # Label column name
  label_column: "label"
  # Test size for train/test split
  test_size: 0.2
  # Random seed
  seed: 42

# Editing configuration
editing:
  # Only local editing is supported
  mode: "local"
  # Whether to enable editing
  enabled: true
  # Number of edits to generate per input
  num_edits: 1
  # Whether to use beam search for generation
  use_beam_search: false
  # Number of beams for beam search
  num_beams: 5
  # Whether to use sampling
  do_sample: true
  # Top-k sampling
  top_k: 50
  # Top-p sampling
  top_p: 0.95
  # Repetition penalty
  repetition_penalty: 1.2
  # Length penalty
  length_penalty: 1.0
  # No repeat n-gram size
  no_repeat_ngram_size: 3
  # Whether to use cache
  use_cache: true
  # Whether to return full sequence
  return_full_text: false
  # Whether to skip special tokens in generation
  skip_special_tokens: true
  # Whether to clean up tokenization spaces
  clean_up_tokenization_spaces: true

# Logging configuration
logging:
  # Log level (debug, info, warning, error, critical)
  level: "info"
  # Log file
  file: "logs/seal.log"
  # Whether to log to console
  console: true
  # Whether to log to file
  file_log: true
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  # Date format
  datefmt: "%Y-%m-%d %H:%M:%S"

# Output configuration
output:
  # Output directory
  dir: "outputs"
  # Whether to save predictions
  save_predictions: true
  # Whether to save metrics
  save_metrics: true
  # Whether to save model
  save_model: true
  # Whether to save tokenizer
  save_tokenizer: true
  # Whether to save config
  save_config: true
  # Whether to save training arguments
  save_training_args: true
  # Whether to save best model
  save_best_model: true
  # Whether to save full model
  save_full_model: true
